{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "### Due Date: Tuesday, October 30, 2018\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "- Training optimal policies / value functions can take very long, so be sure to start solving the problems early to give yourself enough time to finish training everything.\n",
    "- Please save any plots you generate and add them to the notebook. We will not rerun your code for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Direct Policy Algorithms\n",
    "\n",
    "While in previous assignments we focused primarily on value iteration algorithms (e.g. DQN), in this homework we will be implementing direct policy algorithms, which simply learn the parameters of your mapping from states to actions. The structure of these algorithms are essentially the same as those we implemented previously, with the core constituents being a function approximator, a method for computing actions, a method for collecting samples, and a training operation. Accordingly, in order to recycle aspects of the code, we choose to organize these methods into a parent class which may then be abstracted and recycled for the algorithms we implement in subsequent problems.\n",
    "\n",
    "In the below cell, we have instantiated this class and filled in the components you have already written up in homework 2. Spend some time reviewing this class, and then complete the following missing operations:\n",
    "\n",
    "1. Modify the `create_model` method to output a trainable variable (i.e. `tf.get_variable`) that computes the logstd of a certain action in the case of stochastic policies. For this assignment, we will assume that this variable is not dependent on the input state. We will use this in problems 2 and 3.\n",
    "2. Modify in the `action_op` method to calculate and return the actions based on the current parametrization of the policy in the stochastic setting (mean, logstd). The action should be: $\\pi(s) = \\pi_\\text{mean}(s) + exp(logstd) * \\mathcal{N}(0,1)$.\n",
    "3. Test your implemenations in the cells immediately below this class! This should help you do this incrementally.\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- Refer to the `__init__` method when choosing the placeholders and dimensions for your augmentations to the above methods. \n",
    "- Some useful tensorflow classes and methods include: `tf.get_variable`, `tf.exp`, `tf.random_normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DirectPolicyAlgorithm(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 linear=False,\n",
    "                 stochastic=False,\n",
    "                 hidden_size=[64, 64], \n",
    "                 nonlinearity=tf.nn.relu):\n",
    "        \"\"\"Instantiate the policy iteration class.\n",
    "\n",
    "        This initializes the policy model with a set of trainable \n",
    "        parameters, and creates a tensorflow session and a saver \n",
    "        to save checkpoints during training.\n",
    "\n",
    "        In order to train an algorithm using this class, type:\n",
    "\n",
    "            >>> alg = DirectPolicyAlgorithm(...)\n",
    "            >>> alg.train(...)\n",
    "\n",
    "        Note that the \"train\" method is abstract, and needs to be\n",
    "        filled in by a child class.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        env : gym.Env\n",
    "            the environment that the policy will be trained on\n",
    "        linear : bool, optional\n",
    "            specifies whether to use a linear or neural network \n",
    "            policy, defaults to False (i.e. use a fully-connected\n",
    "            network)\n",
    "        stochastic : bool, optional\n",
    "            specifies whether to use a stochastic or deterministic \n",
    "            policy, defaults to False (i.e. deterministic policy)\n",
    "        hidden_size : list of int, optional\n",
    "            list of hidden layers, with each value corresponding \n",
    "            to the number of nodes in that layer \n",
    "        nonlinearity : tf.nn.*\n",
    "            activation nonlinearity\n",
    "        \"\"\"\n",
    "        # clear any previous computation graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # set a random seed\n",
    "        tf.set_random_seed(1234)\n",
    "        \n",
    "        # start a tensorflow session\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        # environment to train on\n",
    "        self.env = env\n",
    "\n",
    "        # number of elements in the action space\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "        # number of elements in the observation space\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "        # mean state (substracted from the state before computing \n",
    "        # actions). This is used in problem 3.\n",
    "        self.mean_state = np.zeros(self.obs_dim)\n",
    "\n",
    "        # actions placeholder\n",
    "        self.a_t_ph = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None, self.ac_dim])\n",
    "        # state placeholder\n",
    "        self.s_t_ph = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None, self.obs_dim])\n",
    "        # expected reward placeholder\n",
    "        self.rew_ph = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None])\n",
    "\n",
    "        # specifies whether the policy is stochastic\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "        # policy that the agent executes during training/testing\n",
    "        self.policy = self.create_model(\n",
    "            args={\n",
    "                \"num_actions\": self.ac_dim,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"linear\": linear,\n",
    "                \"nonlinearity\": nonlinearity,\n",
    "                \"stochastic\": stochastic,\n",
    "                \"scope\": \"policy\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.symbolic_action = self.action_op()\n",
    "\n",
    "        # initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # create saver to save model variables\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def create_model(self, args):\n",
    "        \"\"\"Create a model for your policy or other components.\n",
    "\n",
    "        This model may be linear or a fully-connected network. In\n",
    "        addition, a logstd variable may be specified if the policy\n",
    "        is stochastic, otherwise, the logstd output is set to None.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dict\n",
    "            model-specific arguments, with keys:\n",
    "              - \"stochastic\": a boolean operator that specifies\n",
    "                whether the policy is meant to be stoachstic or\n",
    "                deterministic. If it is stochastic, an additional\n",
    "                trainable variable is created to compute the logstd \n",
    "                of an action given. This variable is not dependent \n",
    "                on the input state.\n",
    "              - \"hidden_size\": a list that specified the shape \n",
    "                of the neural networ (if \"linear\" is False)\n",
    "              - \"num_actions\" number of output actions\n",
    "              - \"scope\": scope of the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Variable\n",
    "            mean actions of the policy\n",
    "        tf.Variable or None\n",
    "            logstd of the policy actions. If the policy is deterministic,\n",
    "            this term is None\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(args[\"scope\"]):\n",
    "            # create the hidden layers\n",
    "            last_layer = self.s_t_ph\n",
    "            for i, hidden_size in enumerate(args[\"hidden_size\"]):\n",
    "                last_layer = tf.layers.dense(\n",
    "                    inputs=last_layer,\n",
    "                    units=hidden_size,\n",
    "                    activation=args[\"nonlinearity\"])\n",
    "\n",
    "            # create the output layer\n",
    "            output_mean = tf.layers.dense(\n",
    "                inputs=last_layer,\n",
    "                units=args[\"num_actions\"],\n",
    "                activation=None)\n",
    "\n",
    "        if args[\"stochastic\"]:\n",
    "            ################################################################\n",
    "            # Create a trainable variable whose output is the same size    #\n",
    "            # as the action space. This variable will represent the output #\n",
    "            # log standard deviation of your stochastic policy.            #\n",
    "            #                                                              #\n",
    "            # Refer to the __init__ method in when choosing an appropriate #\n",
    "            # shape of your variable.                                      #\n",
    "            #                                                              #\n",
    "            # Note: To create this variable, use the `tf.get_variable`     #\n",
    "            # function.                                                    #\n",
    "            ################################################################\n",
    "            output_logstd =  ### FILL IN ###\n",
    "        else:\n",
    "            output_logstd = None\n",
    "\n",
    "        return output_mean, output_logstd\n",
    "    \n",
    "    def action_op(self):\n",
    "        \"\"\"Create a symbolic expression that will be used to compute \n",
    "        actions from observations.\n",
    "\n",
    "        If the policy is determistic, the action is simply dictated by\n",
    "        the output of the policy mean.\n",
    "\n",
    "        Alternatively, if the policy is stochastic, the action is:\n",
    "\n",
    "            a_t = output_mean + exp(output_logstd) * z; z ~ N(0,1)\n",
    "        \"\"\"\n",
    "        if self.stochastic:\n",
    "            output_mean, output_logstd = self.policy\n",
    "\n",
    "            ##############################################################\n",
    "            # Implement a stochastic version of computing actions.       #\n",
    "            #                                                            #\n",
    "            # Remember, the action in a stochastic policy represented by #\n",
    "            # a diagonal Gaussian distribution with mean \"M\" and log     #\n",
    "            # standard deviation \"lstd\" is computed as follows:          #\n",
    "            #                                                            #\n",
    "            #     a = M + exp(lstd) * z                                  #\n",
    "            #                                                            #\n",
    "            # where z is a random normal value, i.e. z ~ N(0,1)          #\n",
    "            #                                                            #\n",
    "            # In order to generate numbers from a normal distribution,   #\n",
    "            # use the `tf.random_normal` function.                       #\n",
    "            ##############################################################\n",
    "            symbolic_action = ### FILL IN ###\n",
    "        else:\n",
    "            symbolic_action, _ = self.policy\n",
    "        \n",
    "        return symbolic_action\n",
    "\n",
    "    def compute_action(self, obs):\n",
    "        \"\"\"Returns a list of actions for a given observation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : np.ndarray\n",
    "            observations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            actions by the policy for a given observation\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.symbolic_action, \n",
    "                             feed_dict={self.s_t_ph: obs})\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"Collect samples from one rollout of the policy.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            dictionary conta.ining trajectory information for the rollout,\n",
    "            specifically containing keys for \"state\", \"action\", \n",
    "            \"next_state\", \"reward\", and \"done\"\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        # start a new rollout by resetting the environment and \n",
    "        # collecting the initial state\n",
    "        state =  self.env.reset()\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "\n",
    "            # compute the action given the state\n",
    "            action = self.compute_action([state])\n",
    "            action = action[0]\n",
    "\n",
    "            # advance the environment once and collect the next state, \n",
    "            # reward, done, and info parameters from the environment\n",
    "            next_state, reward, done, info =  self.env.step(action)\n",
    "\n",
    "            # add to the samples list\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # if the environment returns a True for the done parameter,\n",
    "            # end the rollout before the time horizon is met\n",
    "            if done or steps > env._max_episode_steps:\n",
    "                break\n",
    "\n",
    "        # create the output trajectory\n",
    "        trajectory = {\"state\": np.array(states, dtype=np.float32),\n",
    "                      \"reward\": np.array(rewards, dtype=np.float32),\n",
    "                      \"action\": np.array(actions, dtype=np.float32),\n",
    "                      \"next_state\": np.array(next_states, dtype=np.float32),\n",
    "                      \"done\": np.array(dones, dtype=np.float32)}\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "    def train(self, args):\n",
    "        \"\"\"Abstract training method.\n",
    "\n",
    "        This method will be filled in by algorithm-specific\n",
    "        training operations in subsequent problems.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : dict\n",
    "            algorithm-specific hyperparameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        \"\"\"Save a checkpoint for later viewing.\"\"\"\n",
    "        current_dir = os.getcwd()\n",
    "        save_loc = os.path.join(current_dir, filename)\n",
    "        self.saver.save(self.sess, save_loc)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        \"\"\"Load the model from a specific checkpoint.\"\"\"\n",
    "        current_dir = os.getcwd()\n",
    "        save_loc = os.path.join(current_dir, filename)\n",
    "        self.saver.restore(self.sess, save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test your implementation of the **stochastic policy**, run the below cell.\n",
    "\n",
    "Note that if you are receiving the warning:\n",
    "    \n",
    "    WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
    "\n",
    "don't worrry, this is meant to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hw3_tests import test_1_stochastic\n",
    "test_1_stochastic(DirectPolicyAlgorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test your implementation of **sampling actions** from the stochastic policy, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3_tests import test_1_stochastic_sampling\n",
    "test_1_stochastic_sampling(DirectPolicyAlgorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Policy Gradient Methods\n",
    "\n",
    "In this section, we will implement the likelihood ratio (or REINFORCE) algorithm presented in the lectures. As a review, the objective here (and in all direct policy algorithms) is optimize the parameters $\\theta$ of some policy $\\pi_\\theta$ so that the expected return\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\mathbb{E} \\bigg\\{ \\sum_{t=0}^T \\gamma^t r(t) \\bigg\\}\n",
    "\\end{equation}\n",
    "\n",
    "is optimized. In this algorithm, this is done by calculating the gradient $\\nabla_\\theta J$ and applying a gradient descent method to find a better policy.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta ' = \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "In order compute this gradient, we generate trajectories $\\tau$ from a system by rollouts within an an environment characterized a model transition probability $p$, i.e. $\\tau \\sim p_\\theta(\\tau) = p(\\tau|\\theta)$ with discounted return $r(\\tau) = \\sum_{i=0}^T \\gamma^ir_i$. The expected return is then:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\mathbb{E} \\{ r(\\tau) \\} = \\int_{\\mathbb{T}} p_\\theta(\\tau)r(\\tau) d\\tau\n",
    "\\end{equation}\n",
    "\n",
    "Now, in order to compute the gradient, we apply the log trick and get an expression as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J(\\theta) = \\int_{\\mathbb{T}} \\nabla_\\theta p_\\theta(\\tau)r(\\tau) d\\tau = \\int_{\\mathbb{T}} p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau)r(\\tau) d\\tau = \\mathbb{E} \\{ \\nabla_\\theta \\log p_\\theta(\\tau) r(\\tau) \\}\n",
    "\\end{equation}\n",
    "\n",
    "This expression seems to still contain the model; however, knowing that the probability of a given trajectory can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "p_\\theta(\\tau) = p(s_0) \\prod_{t=0}^Tp(s_{t+1}|s_t,a_t)\\pi_\\theta(a_t|s_t)\n",
    "\\end{equation}\n",
    "\n",
    "The corresponding gradient of this log transition is simply:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta \\log p_\\theta (\\tau) = \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (a_t | s_t)\n",
    "\\end{equation}\n",
    "\n",
    "This means we can rewrite our policy gradient as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) = \\frac{1}{N} \\sum_{i=0}^{N} \\bigg( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\bigg) \\bigg( \\sum_{t=0}^T \\gamma^{t}r_i(t) \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "and the model disappears! Finally, taking into account the causality principle discussed in class, a less variance-prone expression for this gradient is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) = \\frac{1}{N} \\sum_{i=0}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\sum_{t'=t}^T \\gamma^{t'-t}r_i(t')\n",
    "\\end{equation}\n",
    "\n",
    "This final expression should be the major take-away from this review, and what you will be implementing in the subsequent section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. REINFORCE\n",
    "\n",
    "In this section, we will implement the basic form of the REINFORCE algorithm, which goes as follows:\n",
    "\n",
    "1. Collect samples from your current representation of the policy $\\pi_\\theta(s)$ by executing a few rollouts of the environment.\n",
    "2. Compute an estimate for the expected return from any state $s_t$. In order to do so, we will the causality principle, also called *reward-to-go*, which represents the estimated return as the sum of discounted returns starting from the state and moving forward.\n",
    "3. Compute the log-likelihood of each action that was performed by the policy at every given step.\n",
    "4. Estimate and apply a gradient to the training parameters for the policy using the representation of the gradient presented at the start of this problem. \n",
    "5. Repeat steps 1-4 for a certain number of training iterations.\n",
    "\n",
    "The below cell provides the skeleton key for the above algorithm, with four new methods specified: `log_likelihood`, `compute_expected_return`, `update_parameters`, and `apply_gradients`. Fill in the missing elements of this algorithm.\n",
    "\n",
    "**Hint**: Some useful tensorflow classes and methods include: `tf.contrib.distributions.MultivariateNormalDiag`, `tf.reduce_mean`, `tf.multiply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class REINFORCE(DirectPolicyAlgorithm):\n",
    "\n",
    "    def train(self,\n",
    "              num_iterations=100,\n",
    "              steps_per_iteration=1000,\n",
    "              learning_rate=0.001,\n",
    "              gamma=0.95, \n",
    "              **kwargs):\n",
    "        \"\"\"Perform the REINFORE training operation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_iterations : int\n",
    "            number of training iterations\n",
    "        steps_per_iteration : int\n",
    "            number of individual samples collected every training\n",
    "            iteration\n",
    "        learning_rate : float\n",
    "            optimizer learning rate\n",
    "        gamma : float\n",
    "            discount rate\n",
    "        kwargs : dict\n",
    "            additional arguments\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            average return per iteration\n",
    "        \"\"\"\n",
    "        # set the discount as an attribute\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # set the learning rate as an attribute\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # create a symbolic expression to compute the log-likelihoods \n",
    "        log_likelihoods = self.log_likelihoods()\n",
    "\n",
    "        # create a symbolic expression for updating the parameters of \n",
    "        # your policy\n",
    "        #\n",
    "        # Note: the second operator will be used in problem 2.b, please \n",
    "        # ignore when solving 2.a\n",
    "        self.opt, self.opt_baseline = self.define_updates(log_likelihoods)\n",
    "\n",
    "        # initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # average return per training iteration\n",
    "        ret_per_iteration = []\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(num_iterations):\n",
    "            # collect samples from the current policy\n",
    "            samples.clear()\n",
    "            steps_so_far = 0\n",
    "            while steps_so_far < steps_per_iteration:\n",
    "                new_samples = self.rollout()\n",
    "                steps_so_far += new_samples[\"action\"].shape[0]\n",
    "                samples.append(new_samples)\n",
    "\n",
    "            # compute the expected returns\n",
    "            v_s = self.compute_expected_return(samples)\n",
    "\n",
    "            # compute and apply the gradients\n",
    "            self.call_updates(log_likelihoods, samples, v_s, **kwargs)\n",
    "\n",
    "            # compute the average cumulative return per iteration\n",
    "            average_rew = np.mean([sum(s[\"reward\"]) for s in samples])\n",
    "\n",
    "            # display iteration statistics\n",
    "            print(\"Iteration {} return: {}\".format(i, average_rew))\n",
    "            ret_per_iteration.append(average_rew)\n",
    "\n",
    "        return ret_per_iteration\n",
    "\n",
    "    def log_likelihoods(self):\n",
    "        \"\"\"Create a tensorflow operation that computes the log-likelihood \n",
    "        of each performed action.\n",
    "\n",
    "        Remember, the actions in this case are not deterministic, but \n",
    "        rather sampled from a Gaussian distribution; accordingly, there \n",
    "        is a probability associated with each action occuring.\n",
    "        \"\"\"\n",
    "        # tensors representing the mean and standard deviation of\n",
    "        # performing a specific action given a state (these are the \n",
    "        # parameters of your policy)\n",
    "        output_mean, output_logstd = self.policy\n",
    "\n",
    "        ##############################################################\n",
    "        # Create a tf operation to compute the log-likelihood of     #\n",
    "        # each action that was performed by the policy during the    #\n",
    "        # trajectories.                                              #\n",
    "        #                                                            #\n",
    "        # The log likelihood in the continuous case where the policy #\n",
    "        # is expressed by a multivariate gaussian can be computing   #\n",
    "        # using the tensorflow object:                               #\n",
    "        #                                                            #\n",
    "        #    p = tf.contrib.distributions.MultivariateNormalDiag(    #\n",
    "        #        loc=...,                                            #\n",
    "        #        scale_diag=...,                                     #\n",
    "        #    )                                                       #\n",
    "        #                                                            #\n",
    "        # This method takes as input a mean (loc) and standard       #\n",
    "        # deviation (scale_diag), and then can be used to compute    #\n",
    "        # the log-likelihood of a variable as follows:               #\n",
    "        #                                                            #\n",
    "        #    log_likelihoods = p.log_prob(...)                       #\n",
    "        #                                                            #\n",
    "        # For this operation, you will want to use placeholders      #\n",
    "        # created in the __init__ method of problem 1.               #\n",
    "        ##############################################################\n",
    "        log_likelihoods =  ### FILL IN ###\n",
    "\n",
    "        return log_likelihoods\n",
    "\n",
    "    def compute_expected_return(self, samples):\n",
    "        \"\"\"Compute the expected return from a given starting state.\n",
    "\n",
    "        This is done by using the reward-to-go method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : list of list of float\n",
    "            a list of N trajectories, with each trajectory contain T \n",
    "            returns values (one for each step in the trajectory)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of float, or np.ndarray\n",
    "            expected returns for each step in each trajectory\n",
    "        \"\"\"\n",
    "        rewards = [s[\"reward\"] for s in samples]\n",
    "\n",
    "        ##############################################################\n",
    "        # Estimate the expected return from any given starting state #\n",
    "        # using the reward-to-go method.                             #\n",
    "        #                                                            #\n",
    "        # Using this method, the reward is estimated at every step   #\n",
    "        # of the trajectory as follows:                              #\n",
    "        #                                                            #\n",
    "        #   r = sum_{t'=t}^T gamma^(t'-t) * r_{t'}                   #\n",
    "        #                                                            #\n",
    "        # where T is the time horizon at t is the index of the       #\n",
    "        # current reward in the trajectory. For example, for a given #\n",
    "        # set of rewards r = [1,1,1,1] and discount rate gamma = 1,  #\n",
    "        # the expected reward-to-go would be:                        #\n",
    "        #                                                            #\n",
    "        #   v_s = [4, 3, 2, 1]                                       #\n",
    "        #                                                            #\n",
    "        # You will be able to test this in one of the cells below!   #\n",
    "        ##############################################################\n",
    "        v_s = ### FILL IN ###\n",
    "        \n",
    "        return v_s\n",
    "\n",
    "    def define_updates(self, log_likelihoods):\n",
    "        \"\"\"Create a tensorflow operation to update the parameters of \n",
    "        your policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_likelihoods : tf.Operation\n",
    "            the symbolic expression you created to estimate the log \n",
    "            likelihood of a set of actions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Operation\n",
    "            a tensorflow operation for computing and applying the \n",
    "            gradients to the parameters of the policy\n",
    "        None\n",
    "            the second component is used in problem 2.b, please ignore \n",
    "            for this problem\n",
    "        \"\"\"\n",
    "        ##############################################################\n",
    "        # Specify a loss function that can be used to compute the    #\n",
    "        # gradient of denoted at the start of problem 2.             #\n",
    "        #                                                            #\n",
    "        # Note: remember we are trying to **maximize** this value    #\n",
    "        #                                                            #\n",
    "        # For this operation, you will want to use placeholders      #\n",
    "        # created in the __init__ method of problem 1, as well as    #\n",
    "        # the operations provided as inputs to this problem.         #\n",
    "        ##############################################################\n",
    "        loss =  ### FILL IN ###\n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "\n",
    "        return opt, None\n",
    "\n",
    "    def call_updates(self,\n",
    "                     log_likelihoods,\n",
    "                     samples,\n",
    "                     v_s, \n",
    "                     **kwargs):\n",
    "        \"\"\"Apply the gradient update methods in a tensorflow session.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_likelihoods: tf.Operation\n",
    "            the symbolic expression you created to estimate the log \n",
    "            likelihood of a set of actions\n",
    "        samples : list of dict\n",
    "            a list of N trajectories, with each trajectory containing \n",
    "            a dictionary of trajectory data (see self.rollout)\n",
    "        v_s : list of float, or np.ndarray\n",
    "            the estimated expected returns from your\n",
    "            `comput_expected_return` function \n",
    "        kwargs : dict\n",
    "            additional arguments (used in question 3)\n",
    "        \"\"\"\n",
    "        # concatenate the states\n",
    "        states = np.concatenate([s[\"state\"] for s in samples])\n",
    "\n",
    "        # concatenate the actions\n",
    "        actions = np.concatenate([s[\"action\"] for s in samples])\n",
    "\n",
    "        ##############################################################\n",
    "        # Fill in the feed_dict component below to properly execute  #\n",
    "        # the optimization step. Refer to the variables formed in    #\n",
    "        # this problem as well as the __init__ method in problem 1   # \n",
    "        # when doing so.                                             #\n",
    "        ##############################################################\n",
    "        self.sess.run(self.opt, feed_dict={})  ### FILL IN ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test your implementation of the `log_likelihood` method, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3_tests import test_2_log_likelihood\n",
    "test_2_log_likelihood(REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test your implementation of the `compute_expected_return` method, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3_tests import test_2_expected_return\n",
    "test_2_expected_return(REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. State-dependent Baselines\n",
    "\n",
    "The estimates of the gradients for the REINFORCE algorithm presented above have high variance. One possible method for reducing this variance is to subtract a baseline from the advantage function. The new gradient is then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) = \\frac{1}{N} \\sum_{i=0}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\bigg( \\sum_{t'=t}^T \\gamma^{t'-t}r_i(t') - b(s_{it}) \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "As we proved in class, subtracting any state-dependent baselines $b(s_t)$ leads to an unbiased estimator of the gradient. Accordingly, we choose to use a neural network baseline $b_\\phi(s_t)$ that estimates the expected return from any state $s_t$. This network is trained from trajectory data collected during the sample collection procedure to minimize the error between the model estimate of the expected return and the expected return acquired during training. It's loss is accordingly:\n",
    "\n",
    "\\begin{equation}\n",
    "L_\\phi = || b_\\phi(s_t) - \\sum_{t'=t}^T \\gamma^{t'-t}r(t') ||^2\n",
    "\\end{equation}\n",
    "\n",
    "The below cell provides a skeleton key for modifying the REINFORCE class to include a state-dependent baseline represented by a neural network (called `self.baseline`). Modify the `define_updates` and `call_updates` methods in this class.\n",
    "\n",
    "**Hint**: Some useful tensorflow classes and methods include: `tf.reduce_mean`, `tf.multiply`, `tf.square`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_baseline(REINFORCE):\n",
    "\n",
    "    def train(self,\n",
    "              num_iterations=100,\n",
    "              steps_per_iteration=1000,\n",
    "              learning_rate=0.001,\n",
    "              gamma=0.95,\n",
    "              **kwargs):\n",
    "        \"\"\"Perform the REINFORE with baselines training operation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_iterations : int\n",
    "            number of training iterations\n",
    "        steps_per_iteration : int\n",
    "            number of individual samples collected every training\n",
    "            iteration\n",
    "        learning_rate : float\n",
    "            optimizer learning rate\n",
    "        gamma : float\n",
    "            discount rate\n",
    "        kwargs : dict\n",
    "            additional arguments\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            average return per iteration\n",
    "        \"\"\"\n",
    "        # function approximator used for estimated the expected return \n",
    "        # from a given state\n",
    "        self.v_s, _ = self.create_model(\n",
    "            args={\n",
    "                \"num_actions\": 1,\n",
    "                \"hidden_size\": [32, 32],\n",
    "                \"linear\": False,\n",
    "                \"nonlinearity\": tf.nn.relu,\n",
    "                \"stochastic\": False,\n",
    "                \"scope\": \"critic\",\n",
    "            }\n",
    "        )\n",
    "        self.v_s = tf.squeeze(self.v_s)\n",
    "\n",
    "        # the rest of the train method is as before (plus some \n",
    "        # modifications to some of the functions it calls, see below)\n",
    "        return super().train(num_iterations,\n",
    "                             steps_per_iteration, \n",
    "                             learning_rate,\n",
    "                             gamma, \n",
    "                             **kwargs)\n",
    "\n",
    "    def define_updates(self, log_likelihoods):\n",
    "        \"\"\"Create a tensorflow expression for optimizing the parameters \n",
    "        of your neural network policy, as well as the baseline network\n",
    "        that is used to estimate the expected return from a given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_likelihoods : tf.Operation\n",
    "            the symbolic expression you created to estimate the log \n",
    "            likelihood of a set of actions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Operation\n",
    "            a tensorflow operation for computing and applying the \n",
    "            gradients to the parameters of the policy\n",
    "        tf.Operation\n",
    "            a tensorflow operation for computing and applying the \n",
    "            gradients to the parameters of the baseline\n",
    "        \"\"\"\n",
    "        # this will recreate the policy optimization operation from\n",
    "        # section 2.a\n",
    "        opt, _ = super().define_updates(log_likelihoods)\n",
    "\n",
    "        ##############################################################\n",
    "        # Specify the loss function for the baseline.                #\n",
    "        #                                                            #\n",
    "        # The baseline is supposed to accurately approximate the     #\n",
    "        # expect return values given state information.              #\n",
    "        #                                                            #\n",
    "        # Using self.rew_ph for the actual expected returns and      #\n",
    "        # self.v_s for the estimated expected return values.         #\n",
    "        ##############################################################\n",
    "        loss_baseline =  ### FILL IN ###\n",
    "\n",
    "        opt_baseline = tf.train.AdamOptimizer(self.learning_rate).minimize(\n",
    "            loss_baseline)\n",
    "\n",
    "        return opt, opt_baseline\n",
    "\n",
    "    def call_updates(self,\n",
    "                     log_likelihoods,\n",
    "                     samples,\n",
    "                     v_s,\n",
    "                     **kwargs):\n",
    "        \"\"\"Apply the gradient update methods in a tensorflow session.\n",
    "\n",
    "        This will now also include applying the gradients to the parameters \n",
    "        of the baseline.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_likelihoods: tf.Operation\n",
    "            the symbolic expression you created to estimate the log \n",
    "            likelihood of a set of actions\n",
    "        samples : list of dict\n",
    "            a list of N trajectories, with each trajectory containing \n",
    "            a dictionary of trajectory data (see self.rollout)\n",
    "        v_s : list of float, or np.ndarray\n",
    "            the estimated expected returns from your\n",
    "            `comput_expected_return` function \n",
    "        kwargs : dict\n",
    "            additional arguments (used in question 3)\n",
    "        \"\"\"\n",
    "        # concatenate the states\n",
    "        states = np.concatenate([s[\"state\"] for s in samples])\n",
    "\n",
    "        # concatenate the actions\n",
    "        actions = np.concatenate([s[\"action\"] for s in samples])\n",
    "\n",
    "        ##############################################################\n",
    "        # Compute the baseline values and reparameterize them so     #\n",
    "        # that their mean and standard deviations match those of the #\n",
    "        # estimated returns v_s.                                     #\n",
    "        #                                                            #\n",
    "        # Then create a new \"advantage\" term:                        #\n",
    "        #                                                            #\n",
    "        #     advantages = expected_return - baseline_val            #\n",
    "        ##############################################################\n",
    "        advantages =  ### FILL IN ###\n",
    "\n",
    "        ##############################################################\n",
    "        # Fill in the feed_dict component below to properly execute  #\n",
    "        # the policy optimization step. This time, use your computed #\n",
    "        # advantages instead of the expected returns.                #\n",
    "        ##############################################################\n",
    "        self.sess.run(self.opt, feed_dict={})  ### FILL IN ###\n",
    "\n",
    "        ##############################################################\n",
    "        # Fill in the feed_dict component of below to properly       #\n",
    "        # execute the baselien optimization step. Refer to the       #\n",
    "        # variables formed in this problem as well as the __init__   #\n",
    "        # method in problem 1 when doing so.                         #\n",
    "        ##############################################################\n",
    "        self.sess.run(self.opt_baseline, feed_dict={})  ### FILL IN ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Testing your algorithm\n",
    "\n",
    "When you are ready, test your policy gradient algorithms on the *Pendulum-v0* environment in the cell below. Note that this cell runs the algorithm twice: once in the absence of baselines and another time in the presence of them, and then plots the training curves for the two algorithms. Discuss the performance of the algorithms.\n",
    "\n",
    "**Note**: Your best policy should get a max return of a bit over -200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set this number as 1 for testing your algorithm, and 3 for plotting\n",
    "NUM_RUNS = 3\n",
    "\n",
    "# ===========================================================================\n",
    "# Do not modify below here\n",
    "# ===========================================================================\n",
    "\n",
    "# we will test the algorithms on the Pendulum-v0 gym environment\n",
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "# train on the REINFORCE algorithm\n",
    "import numpy as np\n",
    "r = []\n",
    "for i in range(NUM_RUNS):\n",
    "    print(\"\\n==== Training Run {} ====\".format(i))\n",
    "    alg = REINFORCE(env, stochastic=True)\n",
    "    res = alg.train(learning_rate=0.01, gamma=0.95, steps_per_iteration=15000)\n",
    "    r.append(np.array(res))\n",
    "    alg = None\n",
    "\n",
    "# save results\n",
    "np.savetxt(\"InvertedPendulum_results_R.csv\", np.array(r), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set this number as 1 for testing your algorithm, and 3 for plotting\n",
    "NUM_RUNS = 3\n",
    "\n",
    "# ===========================================================================\n",
    "# Do not modify below here\n",
    "# ===========================================================================\n",
    "\n",
    "# we will test the algorithms on the Pendulum-v0 gym environment\n",
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "# train on the REINFORCE with baselines algorithm\n",
    "import numpy as np\n",
    "r = []\n",
    "for i in range(NUM_RUNS):\n",
    "    print(\"\\n==== Training Run {} ====\".format(i))\n",
    "    alg = REINFORCE_baseline(env, stochastic=True)\n",
    "    res = alg.train(learning_rate=0.01, gamma=0.95, steps_per_iteration=15000)\n",
    "    r.append(np.array(res))\n",
    "    alg = None\n",
    "\n",
    "# save results\n",
    "np.savetxt(\"InvertedPendulum_results_Rb.csv\", np.array(r), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect saved results\n",
    "import numpy as np\n",
    "r1 = np.genfromtxt(\"InvertedPendulum_results_R.csv\", delimiter=\",\")\n",
    "r2 = np.genfromtxt(\"InvertedPendulum_results_Rb.csv\", delimiter=\",\")\n",
    "all_results = [r1, r2]\n",
    "labels = [\"REINFORCE\", \"REINFORCE with baselines\"]\n",
    "\n",
    "# plot the results\n",
    "%matplotlib inline\n",
    "from hw3_utils import plot_results\n",
    "fig = plot_results(all_results, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Actor-Critic Algorithms\n",
    "\n",
    "The actor-critic algorithm presented in class is a natural extension to the REINFORCE algorithm with state-dependent baselines that you implemented in section 2.b. Given that the baseline we used in that problem is exactly the value function (i.e. the estimate of expected future returns from a start state), the $b_\\phi (s_t)$ can simply be substituted with a $V_\\phi^\\pi(s_t)$ term and the gradient is then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) = \\frac{1}{N} \\sum_{i=0}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\bigg( \\sum_{t'=t}^T \\gamma^{t'-t}r_i(t') - V_\\phi^\\pi(s_{it}) \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Moreover, estimated the expected return from a state as the reward at that state plus the estimate from the value function at the next state, the policy gradient for the actor-critic method is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) = \\frac{1}{N} \\sum_{i=0}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\bigg( r_i(t) + \\gamma (1-d_{it}) V_\\phi^\\pi(s_{i,t+1}) - V_\\phi^\\pi(s_{it})\\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Moreover, to update this value function estimate, we will now use our previous estimate of the value function, and the loss associated with it is then:\n",
    "\n",
    "\\begin{equation}\n",
    "L_\\phi = || V_\\phi^\\pi(s_t) - \\big(r(t) + \\gamma (1-d) V_\\phi^\\pi(s_{t+1}) \\big) ||^2\n",
    "\\end{equation}\n",
    "\n",
    "In the below cell, we modify the REINFORCE_baseline algorithm to support the actor-critic method mentioned above. Fill in the missing components (note that some of these components simply need to be copied over from the previous question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(REINFORCE_baseline):\n",
    "\n",
    "    def train(self,\n",
    "              num_iterations=100,\n",
    "              steps_per_iteration=1000,\n",
    "              learning_rate=0.001,\n",
    "              gamma=0.95,\n",
    "              **kwargs):\n",
    "        \"\"\"Perform the REINFORE with baselines training operation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_iterations : int\n",
    "            number of training iterations\n",
    "        steps_per_iteration : int\n",
    "            number of individual samples collected every training\n",
    "            iteration\n",
    "        learning_rate : float\n",
    "            optimizer learning rate\n",
    "        gamma : float\n",
    "            discount rate\n",
    "        kwargs : dict\n",
    "            additional arguments, containing:\n",
    "            - \"num_value_estimates\": blank\n",
    "            - \"num_train_steps_per_estimate: blank\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of float\n",
    "            average return per iteration\n",
    "        \"\"\"        \n",
    "        return super().train(num_iterations=num_iterations,\n",
    "                             steps_per_iteration=steps_per_iteration,\n",
    "                             learning_rate=learning_rate,\n",
    "                             gamma=gamma,\n",
    "                             **kwargs)\n",
    "\n",
    "    def compute_expected_return(self, samples):\n",
    "        \"\"\"Compute the expected return from a given starting state.\n",
    "\n",
    "        This is done by using the reward-to-go method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : list of dict\n",
    "            trajectory information that was obtained while collecting \n",
    "            during this training iteration\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of float, or np.ndarray\n",
    "            expected returns for each step in each trajectory\n",
    "        \"\"\"\n",
    "        # rewards from the trajectories\n",
    "        rewards = np.concatenate([s[\"reward\"] for s in samples])\n",
    "        # states from the trajectories\n",
    "        states = np.concatenate([s[\"state\"] for s in samples])\n",
    "        # next states from the trajectories\n",
    "        next_states = np.concatenate([s[\"next_state\"] for s in samples])\n",
    "        # done masks from the trajectories\n",
    "        done = np.concatenate([s[\"done\"] for s in samples])\n",
    "\n",
    "        ##############################################################\n",
    "        # Compute the expected returns.                              #\n",
    "        #                                                            #\n",
    "        # For this, we will assume that the expected return starting #\n",
    "        # from the next state is the estimate of the return by our   #\n",
    "        # baseline in section 2.b (i.e. self.v_s). Denoting this     #\n",
    "        # function as V, and continuing to use the baseline          #\n",
    "        # subtraction approach for reducing variance, the advantage  #\n",
    "        # is expressed as:                                           #\n",
    "        #                                                            #\n",
    "        #     adv = r_t + gamma * (1 - done_t) * V(s_{t+1})          #\n",
    "        #                                                            #\n",
    "        # where t is the current time index.                         #\n",
    "        #                                                            #\n",
    "        # Note: Compute these values explicity (do not return        #\n",
    "        # tensorflow objects).                                       #\n",
    "        ##############################################################\n",
    "        v_s =  ### FILL IN ###\n",
    "        \n",
    "        return v_s\n",
    "\n",
    "    def call_updates(self,\n",
    "                     log_likelihoods,\n",
    "                     samples,\n",
    "                     v_s,\n",
    "                     **kwargs):\n",
    "        \"\"\"Apply the gradient update methods in a tensorflow session.\n",
    "\n",
    "        This will now also include applying the gradients to the parameters \n",
    "        of the baseline.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_likelihoods: tf.Operation\n",
    "            the symbolic expression you created to estimate the log \n",
    "            likelihood of a set of actions\n",
    "        states : list of list of np.ndarray\n",
    "            a list of N trajectories, with each trajectory contain T \n",
    "            state values (one for each step in the trajectory)\n",
    "        actions : list of list of np.ndarray\n",
    "            a list of N trajectories, with each trajectory contain T \n",
    "            actions values (one for each step in the trajectory)\n",
    "        v_s : list of float, or np.ndarray\n",
    "            the estimated expected returns from your\n",
    "            `compute_expected_return` function \n",
    "        kwargs : dict\n",
    "            additional arguments (used in question 3)\n",
    "        \"\"\"\n",
    "        # some variables for actor-critic (see the docstring in the \n",
    "        # train method)\n",
    "        num_value_estimates = kwargs[\"num_value_estimates\"]\n",
    "        num_train_steps_per_estimate = kwargs[\"num_train_steps_per_estimate\"]\n",
    "\n",
    "        # concatenate the rewards\n",
    "        rewards = np.concatenate([s[\"reward\"] for s in samples])\n",
    "\n",
    "        # concatenate the states\n",
    "        states = np.concatenate([s[\"state\"] for s in samples])\n",
    "\n",
    "        # concatenate the next states\n",
    "        next_states = np.concatenate([s[\"next_state\"] for s in samples])\n",
    "\n",
    "        # concatenate the actions\n",
    "        actions = np.concatenate([s[\"action\"] for s in samples])\n",
    "\n",
    "        # concatenate the done masks\n",
    "        dones = np.concatenate([s[\"done\"] for s in samples])\n",
    "\n",
    "        #################################################################\n",
    "        # Call the policy update method as you had done in question 2.b #\n",
    "        #################################################################\n",
    "        advantages =  ### FILL IN ###\n",
    "\n",
    "        self.sess.run(self.opt, feed_dict={})  ### FILL IN ###\n",
    "\n",
    "        ##############################################################\n",
    "        # Use the bootstrapped estimates of the expected return to   #\n",
    "        # modify the parameters of your critic self.v_s.             #\n",
    "        #                                                            #\n",
    "        # In order to do so, first use your current parametrization  #\n",
    "        # of the value function to estimate the expected future      #\n",
    "        # return. Then, for a number of steps, update the parameters #\n",
    "        # of your value function based on the loss described at the  #\n",
    "        # start of this problem.                                     #\n",
    "        ##############################################################\n",
    "        for _ in range(num_value_estimates):\n",
    "            target =  ### FILL IN ###\n",
    "            for _ in range(num_train_steps_per_estimate):\n",
    "                self.sess.run(self.opt_baseline,\n",
    "                              feed_dict={self.rew_ph: target,\n",
    "                                         self.s_t_ph: states})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are ready, test your actor-critic algorithm on the *Pendulum-v0* environment in the cell below.\n",
    "\n",
    "**Note**: Your best policy should get a max return of a bit over -150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set this number as 1 for testing your algorithm, and 3 for plotting\n",
    "NUM_RUNS = 3\n",
    "\n",
    "# ===========================================================================\n",
    "# Do not modify below here\n",
    "# ===========================================================================\n",
    "\n",
    "# we will test the algorithms on the InvertedPendulum-v2 gym environment\n",
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "# train on the Actor Critic algorithm\n",
    "import numpy as np\n",
    "r = []\n",
    "for i in range(NUM_RUNS):\n",
    "    print(\"\\n==== Training Run {} ====\".format(i))\n",
    "    alg = ActorCritic(env, stochastic=True)\n",
    "    res = alg.train(learning_rate=0.01, gamma=0.95, steps_per_iteration=15000,\n",
    "                    num_value_estimates=10, num_train_steps_per_estimate=10)\n",
    "    r.append(np.array(res))\n",
    "    alg = None\n",
    "\n",
    "# save results\n",
    "np.savetxt(\"InvertedPendulum_results_AC.csv\", np.array(r), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect saved results\n",
    "import numpy as np\n",
    "r = np.genfromtxt(\"InvertedPendulum_results_AC.csv\", delimiter=\",\")\n",
    "all_results = [r]\n",
    "labels = [\"Actor Critic\"]\n",
    "\n",
    "# plot the results\n",
    "%matplotlib inline\n",
    "from hw3_utils import plot_results\n",
    "fig = plot_results(all_results, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Learning \"Stabilizing the Ring\" Strategies\n",
    "\n",
    "In questions 2 and 3, we developed a few algorithms that can be used to solve continuous control problems. We now attempt to use these algorithms to solve a widely studied problem, and one that has been frequently referenced in class as well as previous homeworks: stabilizing traffic in a ring road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Designing an environment\n",
    "\n",
    "We begin by designing an MDP that is representative of our problem. Using the Flow computational framework, this can be done by creating an environment object, similar to the one used to trained an optimal traffic light policy in the previous assignment.\n",
    "\n",
    "In the below cell, design an environment than can be used to develop a controller that can dissipate the formation of stop-and-go waves in a ring road. In order to do so, perform the following tasks:\n",
    "\n",
    "1. Modify the `observation_space` and `get_state` methods so that your state is the speed of the RL vehicle, as well the RL vehicle's headway and the speed of the vehicle ahead of it (i.e. its leader). The headway values in the state space should be clipped from above by `self.headway_normalizer` term. Moreover, the speeds and headways in the state space should be divided by the `self.speed_normalizer` and `self.headway_noramlizer` terms, respectively.\n",
    "2. Modify the `action_space` and `_apply_rl_actions` methods so that your policy's actions are converted to desired accelerations by the RL vehicle in the environment. The actions as defined in the `action_space` method should *not* be bounded.\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "- For a review of creating custom environments in flow, please see the following [tutorial](https://github.com/flow-project/flow/blob/master/tutorials/tutorial06_environments.ipynb).\n",
    "- Individual vehicle state information can be collected from the Vehicles class within an environmnet (called by `self.vehicles`). Refer to [this file](https://github.com/flow-project/flow/blob/master/flow/core/vehicles.py) for what sort of information can be collected. The same could be done for scenario/network information using the variable `self.scenario`, which the associated get methods available [here](https://github.com/flow-project/flow/blob/master/flow/scenarios/base_scenario.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.envs import Env   # base environment class\n",
    "from gym.spaces import Box  # used to define configuration of the state/action spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StabilizingTheRingEnv(Env):\n",
    "    \"\"\"Environment used to train ring stabilizing behavior.\n",
    "\n",
    "    States\n",
    "        The states are the speeds and headways of all vehicles.\n",
    "\n",
    "    Actions\n",
    "        The actions are an acceleration for each automated vehicle.\n",
    "\n",
    "    Rewards\n",
    "        The reward function rewards high average speeds from all vehicles in\n",
    "        the network, and penalizes accelerations by the rl vehicle.\n",
    "\n",
    "    Termination\n",
    "        The rollout is terminate if the time horizon is met or if two \n",
    "        vehicles collide into one another.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_params, sumo_params, scenario):\n",
    "        super().__init__(env_params, sumo_params, scenario)\n",
    "\n",
    "        # constant to be divided from all speed values in the state space\n",
    "        self.speed_normalizer = 30\n",
    "\n",
    "        # constant to be divided from all headways in the state space\n",
    "        self.headway_normalizer = 50\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        ##############################################################\n",
    "        # specify dimensions and properties of the action space here #\n",
    "        ##############################################################\n",
    "        return  ### FILL IN ###\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        #############################################################\n",
    "        # specify dimensions and properties of the state space here #\n",
    "        #############################################################\n",
    "        return  ### FILL IN ###\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        ####################################\n",
    "        # specify desired state space here #\n",
    "        ####################################\n",
    "        return  ### FILL IN ###\n",
    "\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        #####################################\n",
    "        # specify desired action space here #\n",
    "        #####################################\n",
    "        ### FILL IN ###\n",
    "\n",
    "    def compute_reward(self, state, rl_actions, **kwargs):\n",
    "        # in the warmup steps\n",
    "        if rl_actions is None:\n",
    "            return 0\n",
    "\n",
    "        vel = np.array([\n",
    "            self.vehicles.get_speed(veh_id)\n",
    "            for veh_id in self.vehicles.get_ids()\n",
    "        ])\n",
    "\n",
    "        if any(vel < -100) or kwargs[\"fail\"]:\n",
    "            return 0.\n",
    "\n",
    "        # reward average velocity\n",
    "        eta_2 = 4.\n",
    "        reward = eta_2 * np.mean(vel) / 20\n",
    "\n",
    "        # punish accelerations (should lead to reduced stop-and-go waves)\n",
    "        eta = 8\n",
    "        rl_actions = np.array(rl_actions)\n",
    "        accel_threshold = 0\n",
    "\n",
    "        if np.mean(np.abs(rl_actions)) > accel_threshold:\n",
    "            reward += eta * (accel_threshold - np.mean(np.abs(rl_actions)))\n",
    "\n",
    "        return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Training and testing\n",
    "\n",
    "Once your environment is ready, you can train your environment by running the scripts below. Each cell is responsible for running a different algorithm, and will save the data and model from the most recent run, so feel free to play around with the hyperparameters of each algorithm independently. After you have trained everything, be sure to plot the training curves from all experiments by executing the last cell in the group.\n",
    "\n",
    "**Note**: Your best policy should get a max return greater than 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating the gym environment\n",
    "from hw3_utils import get_params, HORIZON\n",
    "\n",
    "sumo_params, env_params, scenario = get_params()\n",
    "\n",
    "env = StabilizingTheRingEnv(env_params=env_params, \n",
    "                            sumo_params=sumo_params, \n",
    "                            scenario=scenario)\n",
    "env._max_episode_steps = HORIZON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### training on REINFORCE\n",
    "import numpy as np\n",
    "alg = REINFORCE(env, stochastic=True)\n",
    "\n",
    "# feel free to modify the hyperparameters\n",
    "cum_rewards = alg.train(learning_rate=0.1, gamma=0.99,\n",
    "                        steps_per_iteration=3000, num_iterations=300)\n",
    "\n",
    "alg.save_checkpoint(\"REINFORCE.ckpt\")\n",
    "np.savetxt(\"REINFORCE.csv\", cum_rewards, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### training on REINFORCE with a baseline\n",
    "import numpy as np\n",
    "alg = REINFORCE_baseline(env, stochastic=True)\n",
    "\n",
    "# feel free to modify the hyperparameters\n",
    "cum_rewards = alg.train(learning_rate=0.1, gamma=0.99,\n",
    "                        steps_per_iteration=3000, num_iterations=300)\n",
    "\n",
    "alg.save_checkpoint(\"REINFORCE_baseline.ckpt\")\n",
    "np.savetxt(\"REINFORCE_baseline.csv\", cum_rewards, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### training on Actor Critic\n",
    "import numpy as np\n",
    "alg = ActorCritic(env, stochastic=True)\n",
    "\n",
    "# feel free to modify the hyperparameters\n",
    "cum_rewards = alg.train(learning_rate=0.1, gamma=0.99, \n",
    "                        steps_per_iteration=3000, num_iterations=300,\n",
    "                        num_value_estimates=10, num_train_steps_per_estimate=10)\n",
    "\n",
    "alg.save_checkpoint(\"ActorCritic.ckpt\")\n",
    "np.savetxt(\"ActorCritic.csv\", cum_rewards, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting results\n",
    "import numpy as np\n",
    "from hw3_utils import plot_results\n",
    "\n",
    "res_REINFORCE = np.array([np.genfromtxt(\"REINFORCE.csv\")])\n",
    "res_REINFORCE_baseline = np.array([np.genfromtxt(\"REINFORCE_baseline.csv\")])\n",
    "res_Actor_Critic = np.array([np.genfromtxt(\"ActorCritic.csv\")])\n",
    "all_results = [res_REINFORCE, res_REINFORCE_baseline, res_Actor_Critic]\n",
    "labels = [\"REINFORCE\", \"REINFORCE Baseline\", \"Actor Critic\"]\n",
    "\n",
    "plot_results(all_results, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test that at least one of your algorithms has successfully managed to dissipate the formation of stop and go waves in the ring road network. In order to do so, specify the policy that performed the best from your implementations above and rerun it on the network in the cell below. If your training algorithm was successful, the average speed of vehicles should remain near the higher dotted line in the generated figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Specify the algorithm you would like to visualize here. #\n",
    "#                                                         #\n",
    "# Choose one of:                                          #\n",
    "# - \"REINFORCE\"                                           #\n",
    "# - \"REINFORCE_baseline\"                                  #\n",
    "# - \"ActorCritic\"                                         #\n",
    "###########################################################\n",
    "alg_name = \"ActorCritic\"\n",
    "\n",
    "# ===========================================================================\n",
    "# Do not modify below here\n",
    "# ===========================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hw3_utils import get_params, HORIZON\n",
    "\n",
    "assert alg_name in [\"REINFORCE\", \"REINFORCE_baseline\", \"ActorCritic\"]\n",
    "\n",
    "# modify the reward function of the environment to output average speeds:\n",
    "class TestEnvironment(StabilizingTheRingEnv):\n",
    "    def compute_reward(self, state, rl_actions, **kwargs):\n",
    "        return np.mean(self.vehicles.get_speed(self.vehicles.get_ids()))\n",
    "\n",
    "# create the gym environment\n",
    "sumo_params, env_params, scenario = get_params(render=True)\n",
    "env = TestEnvironment(env_params=env_params, \n",
    "                      sumo_params=sumo_params, \n",
    "                      scenario=scenario)\n",
    "env._max_episode_steps = HORIZON\n",
    "\n",
    "# run the environment for one rollout with the trained policy\n",
    "alg = DirectPolicyAlgorithm(env, stochastic=True)\n",
    "alg.load_checkpoint(alg_name + \".ckpt\")\n",
    "samples = alg.rollout()\n",
    "\n",
    "# plot the results from that rollout\n",
    "avg_speeds = samples[\"reward\"]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Stabilizing Controller Performance\", fontsize=25)\n",
    "plt.xlabel(\"time step\", fontsize=20)\n",
    "plt.ylabel(\"average speed (m/s)\", fontsize=20)\n",
    "plt.plot(avg_speeds, c='k', label=\"average speed with one AV\")\n",
    "plt.plot([4.82] * len(avg_speeds), '--', c='b', label=\"non-perturbed equilibrium\")\n",
    "plt.plot([3.28] * len(avg_speeds), '--', c='r', label=\"perturbed equilibrium\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Effect of Hyperparameters (BONUS)\n",
    "\n",
    "Explore the effect of one or more hyperparamters (e.g. depth or width of your neural network, learning rate, etc...) on the training performance of one of your algorithms on the above task. Plot the training results for each choice hyperparameter in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#### Perform hyperparameter search here ####\n",
    "############################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
